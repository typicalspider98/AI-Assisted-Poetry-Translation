import redis
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel

# è¿æ¥ Redis
r = redis.Redis(host="localhost", port=6379, db=2)

# åŠ è½½ BGE æ¨¡å‹
MODEL_PATH = "./models/bge-base-en-v1.5"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModel.from_pretrained(MODEL_PATH)


def get_embedding(text):
    """
    è®¡ç®—æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼ˆå‡å€¼æ± åŒ–ï¼‰
    """
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        output = model(**tokens)
    embedding = output.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embedding


def search_topk_similar(query, top_k=5):
    """
    åœ¨ Redis ä¸­æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„ top_k è¯æ¡
    """
    query_vector = get_embedding(query)  # è®¡ç®—æŸ¥è¯¢çš„å‘é‡
    words = r.keys("*")  # è·å–æ‰€æœ‰å­˜å‚¨çš„ key
    similarities = []

    for word in words:
        stored_vector = np.frombuffer(r.get(word), dtype=np.float32)  # è¯»å– Redis å­˜å‚¨çš„å‘é‡
        similarity = np.dot(query_vector, stored_vector)  # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities.append((word.decode("utf-8"), similarity))

    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]  # è¿”å› top_k ç»“æœ


if __name__ == "__main__":
    query = "I like wear gumboot"  # æµ‹è¯•æŸ¥è¯¢è¯
    # query = "bright moon"  # æµ‹è¯•æŸ¥è¯¢è¯
    results = search_topk_similar(query, top_k=5)
    print("ğŸ” æœ€ç›¸ä¼¼çš„è¯æ¡:")
    for word, score in results:
        print(f"{word}: {score:.4f}")
