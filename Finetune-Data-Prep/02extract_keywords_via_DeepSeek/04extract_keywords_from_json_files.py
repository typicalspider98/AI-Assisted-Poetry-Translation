# extract_keywords_batch_folder.py

import os
import json
import re
from typing import List, Dict
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from datetime import datetime
from transformers import BitsAndBytesConfig

# ========= æœ¬åœ°æ¨¡å‹è·¯å¾„ =========
MODEL_PATH = "../C2NZE/models/DeepSeek-R1-Distill-Qwen-14B"

# ========= æ—¥å¿—é…ç½® =========
if not os.path.exists("logs"):
    os.makedirs("logs")
log_path = os.path.join("logs", f"extract_keywords_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

def log(message: str):
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(message.strip() + "\n")
    print(message)

# ========= åŠ è½½æ¨¡å‹ =========
log(f"ğŸš€ åŠ è½½æ¨¡å‹ï¼š{MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,
    device_map="auto",
    torch_dtype=torch.float16,
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
    )
)
log("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ========= æ ¸å¿ƒå‡½æ•° =========
def build_keyword_prompt(poem_text: str) -> str:
    return (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ–‡å¤å…¸è¯—æ­Œä¸è‹±è¯­æ–‡åŒ–çš„ç¿»è¯‘é¡¾é—®ã€‚\n"
        "è¯·æ ¹æ®ä¸‹åˆ—è¯—æ­Œå†…å®¹åŠé•¿åº¦æå–é€‚é‡çš„å…³é”®è¯<word>ï¼Œç”¨äºæŒ‡å¯¼è‹±æ–‡ç¿»è¯‘ã€‚\n"
        "è¦æ±‚:\n"
        "1.ä¸è¦é€å¥ç…§æ¬åŸè¯—å¥ï¼Œç»“åˆè¯—æ­ŒåŸæ„å’Œæ„å¢ƒï¼Œæå–å…¶ä¸­å¯ç”¨äºç¿»è¯‘çš„æ ¸å¿ƒä¸»é¢˜è¯ï¼Œä¾‹å¦‚ï¼šåŠ¨è¯ã€åè¯ã€å½¢å®¹è¯ã€å‰¯è¯ç­‰ï¼Œä»¥åŠæ„è±¡å’Œæ–‡åŒ–æ¦‚å¿µã€‚\n"
        "2.åœ¨ä¸ç ´åè¯—æ­Œæœ¬æ„å’Œæ„å¢ƒçš„å‰æä¸‹ï¼Œæå–çš„ä¸»é¢˜è¯<word>ï¼ŒæŒ‰ä¸­æ–‡è¯—æ­ŒéŸµå¾‹å’Œè¯­ä¹‰åœé¡¿åˆ’åˆ†å‡ºçš„æœ€å°è¯­ç´ å’Œè¯è¯­ã€‚\n"
        "3.ä¸»é¢˜è¯<word>åŒ…æ‹¬æ‰€æœ‰æ„è±¡ï¼Œä¸ºç®€æ˜çš„è‹±æ–‡å•è¯ã€‚\n"
        "4.è¿”å› JSON æ ¼å¼ï¼Œå…³é”®è¯åº”å…·æœ‰ç¿»è¯‘ä»·å€¼ä¸æ–‡åŒ–è±¡å¾æ€§ã€‚\n"
        "JSON ç¤ºä¾‹æ ¼å¼å¦‚ä¸‹ï¼š\n"
        "```json\n{\n  \"keywords\": [\n    \"<word>: Simple explanation in English>\",\n    \"<word>: <Simple explanation in English>\",\n"
        "    \"<word>: <Simple explanation in English>\"\n  ]\n}\n```\n\n"
        f"è¯—æ­ŒåŸæ–‡ï¼š{poem_text}"
    )

def call_model(prompt: str, max_new_tokens: int = 2048) -> str:
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        eos_token_id=tokenizer.eos_token_id
    )
    input_len = inputs["input_ids"].shape[-1]
    return tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()

def extract_json_keywords(model_output: str) -> List[str]:
    try:
        match = re.search(r"```json\s*({[\s\S]*?})\s*", model_output)
        if match:
            return json.loads(match.group(1)).get("keywords", [])
        else:
            return []
    except Exception as e:
        log(f"âŒ JSON è§£æå¤±è´¥: {e}")
        return []

def process_single_poem_file(input_path: str, output_path: str):
    with open(input_path, "r", encoding="utf-8") as f:
        poems = json.load(f)

    all_results = []
    for i, poem in enumerate(poems):
        content = poem.get("content", "").strip()
        if not content:
            log(f"âš ï¸ ç¬¬{i+1}é¦–è¯—å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
            continue

        log(f"\nğŸ“œ å¤„ç†ç¬¬{i+1}é¦–ï¼šã€Š{poem.get('title', '')}ã€‹")
        prompt = build_keyword_prompt(content)
        model_output = call_model(prompt)
        log(f"ğŸ” æ¨¡å‹è¾“å‡ºï¼š\n{model_output}")
        keywords = extract_json_keywords(model_output)

        if keywords:
            poem_result = {
                "title": poem.get("title", f"poem_{i+1}"),
                "author": poem.get("author", ""),
                "dynasty": poem.get("dynasty", ""),
                "content": content,
                "keywords": keywords
            }
            all_results.append(poem_result)
            log(f"âœ… æå–å…³é”®è¯ï¼š{keywords}")
        else:
            log(f"âš ï¸ æœªæå–å‡ºå…³é”®è¯")

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    log(f"ğŸ“ å·²ä¿å­˜è‡³ï¼š{output_path}")

def process_poem_folder(input_folder: str, output_folder: str):
    os.makedirs(output_folder, exist_ok=True)
    for filename in os.listdir(input_folder):
        if filename.endswith(".json"):
            input_path = os.path.join(input_folder, filename)
            output_path = os.path.join(output_folder, filename)
            log(f"===============================\nğŸ“‚ æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{filename}")
            process_single_poem_file(input_path, output_path)

# ========= å¯åŠ¨å…¥å£ =========
if __name__ == "__main__":
    process_poem_folder("converted_jsons", "keywords_jsons")
