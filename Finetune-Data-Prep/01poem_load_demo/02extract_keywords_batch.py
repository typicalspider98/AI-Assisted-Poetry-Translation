# extract_keywords_batch.py
# æµ‹è¯•æœ¬åœ°æ¨¡å‹ä»jsonè¯—æ­Œæ–‡æœ¬æå–è¯—æ­Œå…³é”®è¯å¹¶ä¿å­˜
import os
import json
import re
from typing import List, Dict
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from datetime import datetime
from transformers import BitsAndBytesConfig

# ========= æœ¬åœ°æ¨¡å‹è·¯å¾„ =========
MODEL_PATH = "../C2NZE/models/DeepSeek-R1-Distill-Qwen-14B"

# ========= æ—¥å¿—é…ç½® =========
if not os.path.exists("logs"):
    os.makedirs("logs")
log_path = os.path.join("logs", f"extract_keywords_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

def log(message: str):
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(message.strip() + "\n")
    print(message)

# ========= åŠ è½½æ¨¡å‹ =========
log(f"ğŸš€ åŠ è½½æ¨¡å‹ï¼š{MODEL_PATH}")
# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True
    )

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,  # load_in_8bit=True,  # å¯ç”¨ 8-bit é‡åŒ–
    device_map="auto",  # device_map="auto",
    torch_dtype=torch.float16,
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        )
    )
'''
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH, trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
)
'''
log("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ========= æ ¸å¿ƒå‡½æ•° =========
def build_keyword_prompt(poem_text: str) -> str:
    return (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ–‡å¤å…¸è¯—æ­Œä¸è‹±è¯­æ–‡åŒ–çš„ç¿»è¯‘é¡¾é—®ã€‚\n"
        "è¯·æ ¹æ®ä¸‹åˆ—è¯—æ­Œå†…å®¹åŠé•¿åº¦æå–é€‚é‡çš„å…³é”®è¯ï¼Œç”¨äºæŒ‡å¯¼è‹±æ–‡ç¿»è¯‘ã€‚\n"
        "è¦æ±‚:\n"
        "1.ä¸è¦é€å¥ç…§æ¬åŸè¯—å¥ï¼Œç»“åˆè¯—æ­ŒåŸæ„å’Œæ„å¢ƒï¼Œæå–å…¶ä¸­å¯ç”¨äºç¿»è¯‘çš„æ ¸å¿ƒä¸»é¢˜è¯ï¼ŒåŒ…æ‹¬åŠ¨è¯ã€åè¯ã€å½¢å®¹è¯ã€å‰¯è¯ç­‰ï¼Œä»¥åŠæ„è±¡å’Œæ–‡åŒ–æ¦‚å¿µã€‚\n"
        "2.åœ¨ä¸ç ´åè¯—æ­Œæœ¬æ„å’Œæ„å¢ƒçš„å‰æä¸‹ï¼Œæå–çš„ä¸»é¢˜è¯æ˜¯æŒ‰ä¸­æ–‡è¯—æ­ŒéŸµå¾‹å’Œè¯­ä¹‰åœé¡¿åˆ’åˆ†å‡ºçš„æœ€å°è¯­ç´ å’Œè¯è¯­ï¼Œç‰¹æ®Šæƒ…å†µä¸‹ä¸ºä¸ç ´åè¯—æ­Œæ•´ä½“å«ä¹‰ä¹Ÿå¯ä»¥æ˜¯ä¸­æ–‡çŸ­è¯­ã€‚\n"
        "3.ä¸»é¢˜è¯åŒ…æ‹¬æ‰€æœ‰æ„è±¡ï¼Œå¹¶æœ€ç»ˆä»¥å‡†ç¡®ç®€æ˜çš„è‹±æ–‡å•è¯å±•ç¤ºã€‚\n"
        "4.å¹¶è¿”å› JSON æ ¼å¼ï¼Œå…³é”®è¯åº”å…·æœ‰ç¿»è¯‘ä»·å€¼ä¸æ–‡åŒ–è±¡å¾æ€§ã€‚\n"
        "JSON ç¤ºä¾‹æ ¼å¼å¦‚ä¸‹ï¼š\n"
        "```json\n{\n  \"keywords\": [\n    \"word1: English simple explanation\",\n    \"word2: English simple explanation\",\n    \"word3: English simple explanation\"\n  ]\n}\n```\n\n"
        f"è¯—æ­ŒåŸæ–‡ï¼š{poem_text}"
    )

def call_model(prompt: str, max_new_tokens: int = 2048) -> str:
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        eos_token_id=tokenizer.eos_token_id
    )
    input_len = inputs["input_ids"].shape[-1]
    return tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()

def extract_json_keywords(model_output: str) -> List[str]:
    try:
        match = re.search(r"```json\s*({[\s\S]*?})\s*", model_output)
        if match:
            return json.loads(match.group(1)).get("keywords", [])
        else:
            return []  # fallback: ä¸å†é¢å¤–å¤„ç†åŸå§‹è¾“å‡ºï¼Œä¿æŒç®€æ´æ¸…æ™°
    except Exception as e:
        log(f"âŒ JSON è§£æå¤±è´¥: {e}")
        return []

def load_poems(filepath: str) -> List[Dict]:
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)

def extract_keywords_batch(poem_json_path: str, output_path: str):
    poems = load_poems(poem_json_path)
    all_results = []

    for i, poem in enumerate(poems):
        content = poem.get("content", "").strip()
        if not content:
            log(f"âš ï¸ ç¬¬{i+1}é¦–è¯—å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
            continue

        log(f"\nğŸ“œ å¤„ç†ç¬¬{i+1}é¦–ï¼šã€Š{poem.get('title', '')}ã€‹")
        prompt = build_keyword_prompt(content)
        model_output = call_model(prompt)
        log(f"ğŸ” æ¨¡å‹è¾“å‡ºï¼š\n{model_output}")  # ç”¨äºè°ƒè¯•
        keywords = extract_json_keywords(model_output)

        if keywords:
            poem_result = {
                "title": poem.get("title", f"poem_{i+1}"),
                "author": poem.get("author", ""),
                "dynasty": poem.get("dynasty", ""),
                "content": content,
                "keywords": keywords
            }
            all_results.append(poem_result)
            log(f"âœ… æå–å…³é”®è¯ï¼š{keywords}")
        else:
            log(f"âš ï¸ æœªæå–å‡ºå…³é”®è¯")

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    log(f"\nğŸ“ å·²ä¿å­˜æ‰€æœ‰ç»“æœè‡³ï¼š{output_path}")

# ========= å¯åŠ¨è„šæœ¬ =========
if __name__ == "__main__":
    extract_keywords_batch("poems.json", "keywords_all.json")