import json
import torch

import os
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from utils.data_utils import format_dataset, parse_output
'''
pip install datasets
pip install -U peft

'''

# é…ç½®åŠ è½½å‡½æ•°
def load_configs(model_name):
    """åŠ¨æ€åŠ è½½é…ç½®"""
    base_dir = Path(__file__).parent
    
    # åŠ è½½è®­ç»ƒé…ç½®
    with open(base_dir/"configs"/"train_config.json") as f:
        train_config = json.load(f)
    
    # åŠ è½½LoRAé…ç½®
    lora_config_path = base_dir/"configs"/"lora_config.json"
    with open(lora_config_path) as f:
        lora_config = json.load(f)

    # ç§»é™¤éæ³•å­—æ®µï¼ˆå¦‚ _commentï¼‰
    lora_config = {k: v for k, v in lora_config.items() if not k.startswith("_")}
    
    # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
    if "qwen" in model_name.lower():
        lora_config["target_modules"] = ["c_attn", "c_proj", "w1", "w2"]
    elif "llama" in model_name.lower():
        lora_config["target_modules"] = ["q_proj", "v_proj"]
    
    return train_config, LoraConfig(**lora_config)

def main():
    # åŸºç¡€é…ç½®
    MODEL_PATH = "/workspace/AI-Assisted-Poetry-Translation/C2NZE/models/DeepSeek-R1-Distill-Qwen-14B"
    DATA_PATH = "/workspace/AI-Assisted-Poetry-Translation/Finetune-Data-Prep/05fintune_lora/dataset/å”_part1_demo_lora.jsonl"
    OUTPUT_DIR = "output"
    
    # åŠ è½½é…ç½®
    train_config, peft_config = load_configs(MODEL_PATH)
    
    # ==== åˆå§‹åŒ–æ¨¡å‹ ====
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        use_fast=False
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        quantization_config=bnb_config,
        trust_remote_code=True,
        device_map="auto"
    )
    base_model = prepare_model_for_kbit_training(base_model)
    
    # ==== å¤„ç†æ•°æ® ====
    dataset = load_dataset("json", data_files=DATA_PATH, split="train")
    
    formatted_dataset = dataset.map(
        lambda x: format_dataset(x, tokenizer, train_config["max_length"]),
        remove_columns=dataset.column_names
    )
    
    # ==== å‡†å¤‡è®­ç»ƒ ====
    model = get_peft_model(base_model, peft_config)
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {model.print_trainable_parameters()}")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=train_config["per_device_train_batch_size"],
        gradient_accumulation_steps=train_config["gradient_accumulation_steps"],
        learning_rate=train_config["learning_rate"],
        num_train_epochs=train_config["num_train_epochs"],
        logging_steps=train_config["logging_steps"],
        save_strategy=train_config["save_strategy"],
        bf16=train_config["bf16"],
        optim=train_config["optim"],
        max_grad_norm=train_config["max_grad_norm"],
        warmup_ratio=train_config["warmup_ratio"],
        report_to="none"
    )
    
    # åˆå§‹åŒ–Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=formatted_dataset,
        data_collator=lambda data: {
            'input_ids': torch.stack([x['input_ids'] for x in data]),
            'attention_mask': torch.stack([x['attention_mask'] for x in data]),
            'labels': torch.stack([x['labels'] for x in data])
        }
    )
    
    # ==== å¼€å§‹è®­ç»ƒ ====
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {OUTPUT_DIR}")

if __name__ == "__main__":
    main()