import sys
import openai
import torch
from openai import OpenAI  # æ–°ç‰ˆ SDK ç”¨æ³•
from transformers import AutoTokenizer, AutoModelForCausalLM
from datetime import datetime
import os
from transformers import BitsAndBytesConfig

import re
strip_dash_block = lambda s: (
    re.search(r"---\s*([\s\S]+?)\s*---", s).group(1).strip()
    if re.search(r"---\s*([\s\S]+?)\s*---", s)
    else s
)


# é™åˆ¶ PyTorch çš„åˆ†é…ç­–ç•¥ï¼Œä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡åŒ–
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

# translation_logic.py é¡¶éƒ¨åŠ å…¥ï¼ˆæˆ–æ”¾å…¥ä¸“ç”¨æ¨¡å—ä¹Ÿè¡Œï¼‰
import redis
import sys

def check_redis_connection_or_exit(host="localhost", port=6379, db=0):
    try:
        r = redis.Redis(host=host, port=port, db=db)
        r.ping()
        print(f"âœ… Redis æ•°æ®åº“ {db} å·²è¿æ¥")
    except redis.ConnectionError:
        print("âŒ Redis æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Redis æœåŠ¡æ˜¯å¦å·²å¯åŠ¨ã€‚"
        "\n Quick launch:\nnohup dockerd > dockerd.log 2>&1 &\n"
        "docker start redis-server_NZDdictionary")
        sys.exit("âŒ é¡¹ç›®å¯åŠ¨ç»ˆæ­¢ï¼šRedis æ•°æ®åº“ä¸å¯ç”¨ã€‚")

check_redis_connection_or_exit(db=0)
check_redis_connection_or_exit(db=2)

global custom_model_path, tokenizer, model
#####################################
# æ—¥å¿—é…ç½®ï¼šåˆ›å»º logs æ–‡ä»¶å¤¹ï¼Œå¹¶ç”Ÿæˆæ—¥å¿—æ–‡ä»¶
#####################################
if not os.path.exists("logs"):
    os.makedirs("logs")

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = os.path.join("logs", f"output_{timestamp}.md")
log_file = open(log_filename, "w", encoding="utf-8")


def write_log(entry: str):
    """è®°å½•æ—¥å¿—åˆ°æ–‡ä»¶ï¼Œé™„åŠ æ—¶é—´æˆ³"""
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_file.write(f"[{ts}] {entry}\n\n")
    log_file.flush()


# ç”¨äºå­˜å‚¨ç”¨æˆ·æŒ‡å®šçš„æœ¬åœ°æ¨¡å‹è·¯å¾„å’Œ Token
custom_model_path = None
local_model_token = None

# ç”¨ accelerate è®© PyTorch é€æ­¥åŠ è½½æ¨¡å‹
from accelerate import infer_auto_device_map

def set_model_path(new_path: str):
    global custom_model_path, tokenizer, model
    """ è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„ """
    new_path = os.path.abspath(new_path)
    custom_model_path = new_path
    write_log(f"ç”¨æˆ·è®¾ç½®äº†æ–°çš„æ¨¡å‹è·¯å¾„: {new_path}")
    print(f"âœ… [DEBUG] æ¨¡å‹è·¯å¾„å·²æ›´æ–°: {new_path}")  # æ–¹ä¾¿è°ƒè¯•
    # return f"è·¯å¾„å·²ç¡®è®¤: {new_path}"  # âœ… è®© Gradio UI æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
    # åŠ è½½æ–°æ¨¡å‹
    try:
        write_log("å¼€å§‹åŠ è½½æœ¬åœ° Boss æ¨¡å‹...")
        print("Loading local Boss model from local files...")
        # model_path = "../DeepSeek-R1-Distill-Llama-8B"  # è¯·æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
        # model_path = "../DeepSeek-R1-Distill-Qwen-14B"  # è¯·æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
        # model_path = "../Qwen2.5-7B-Instruct"  # è¯·æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
        default_model_path = os.path.abspath("/workspace/Project-Code/AI-Assisted-Poetry-Translation/C2NZE/models/DeepSeek-R1-Distill-Qwen-14B")
        model_path = custom_model_path if custom_model_path else default_model_path
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)

        #model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
        # device_map = infer_auto_device_map(model, max_memory={0: "14GB", "cpu": "8GB"})  
        model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, local_files_only=True,  # load_in_8bit=True,  # å¯ç”¨ 8-bit é‡åŒ–
                                                    device_map="auto",  # device_map="auto",
                                                    torch_dtype=torch.float16,
                                                    quantization_config=BitsAndBytesConfig(
                                                        load_in_4bit=True,
                                                        bnb_4bit_compute_dtype=torch.float16,
                                                        bnb_4bit_quant_type="nf4",
                                                        )
                                                    )
                                                    # torch_dtype="auto")  # åŠç²¾åº¦è®¡ç®—ï¼‰ï¼ŒèŠ‚çœæ˜¾å­˜
        # model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map="cpu")
        print("Local Boss model loaded.")
        write_log("æœ¬åœ° Boss æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {new_path}"  # è¿”å›æˆåŠŸä¿¡æ¯
    except Exception as e:
        write_log(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"


def set_local_model_token(token: str):
    """ è®¾ç½®æœ¬åœ°æ¨¡å‹ Token """
    global local_model_token
    local_model_token = token
    write_log(f"ç”¨æˆ·è®¾ç½®äº†æ–°çš„æœ¬åœ°æ¨¡å‹ Token")
    print(f"âœ… [DEBUG] æ¨¡å‹tokenå·²æ›´æ–°: {token}")  # æ–¹ä¾¿è°ƒè¯•
    return f"è·¯å¾„å·²ç¡®è®¤: {local_model_token}"  # âœ… è®© Gradio UI æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯


#####################################
# 1. åŠ è½½æœ¬åœ° Boss æ¨¡å‹ï¼ˆDeepSeek-R1-Distill-Qwen-14Bï¼‰
#####################################


#####################################
# 2. é…ç½® DeepSeek APIï¼ˆå¤§æ¨¡å‹è°ƒç”¨ï¼‰
#####################################
# è¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ DeepSeek API Key
DEEPSEEK_API_KEY = "sk-e5484044e6314d95b63af7f93a00ea7e"  # TODO: æ›¿æ¢ä¸ºå®é™… API Key
client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")
write_log("DeepSeek API é…ç½®å®Œæˆã€‚")


#####################################
# 3. å®šä¹‰æœ¬åœ°ç”Ÿæˆå‡½æ•°
#####################################
def local_generate(prompt_text: str, max_new_tokens=128, min_length=256):
    global custom_model_path, tokenizer, model
    """
    ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
    å‚æ•°è¯´æ˜ï¼š
        - max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        - min_length: æœ€å°ç”Ÿæˆ token æ•°ï¼Œé˜²æ­¢è¿‡æ—©ç»“æŸ
        - do_sample: å¯ç”¨é‡‡æ ·ï¼Œé¿å…è´ªå¿ƒæœç´¢
        - temperature: æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼ˆæ¨è 0.6ï¼‰
        - top_p: nucleus é‡‡æ ·ï¼ˆä¾‹å¦‚ 0.9ï¼‰
    """
    write_log(f"æœ¬åœ°ç”Ÿæˆè°ƒç”¨ï¼Œè¾“å…¥ prompt:\n{prompt_text}")
    if model is None or tokenizer is None:
        return "âš ï¸ è¿˜æœªåŠ è½½æ¨¡å‹ï¼Œè¯·å…ˆè®¾ç½®æ¨¡å‹è·¯å¾„ï¼"

    # åœ¨ Prompt å¤´éƒ¨æ˜¾å¼åŠ å…¥ <think>\n ä»¥å¼ºåˆ¶æ€ç»´æ¨¡å¼
    # modified_prompt = "<think>\n" + prompt_text
    # inputs = tokenizer(modified_prompt, return_tensors="pt")

    inputs = tokenizer(prompt_text, return_tensors="pt").to("cuda")  # for GPU
    # inputs = tokenizer(prompt_text, return_tensors="pt")  # for CPU
    print("max_new_tokens:", max_new_tokens)
    torch.cuda.empty_cache()  # âœ… æ¸…ç†æ˜¾å­˜ï¼Œéå¼ºåˆ¶ä½†ä¿é™©
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        min_length=min_length,
        do_sample=True,
        # temperature=0.8,
        # top_p=0.9
        temperature=0.7,
        top_p=0.85,
        repetition_penalty=1.1,  # æŠ‘åˆ¶é‡å¤
        eos_token_id=tokenizer.eos_token_id  # æ˜¾å¼æŒ‡å®šç»“æŸç¬¦
    )
    '''
    ä¸è®© result åŒ…å« prompt_textï¼Œåªä¿ç•™æ–°ç”Ÿæˆçš„å†…å®¹
    '''
    input_length = inputs["input_ids"].shape[-1]
    generated_tokens = outputs[0][input_length:]
    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    # åˆ é™¤</think> åŠå…¶å‰é¢çš„å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "</think>" in result:
        result = result.split("</think>")[-1].strip()
    # result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    write_log(f"æœ¬åœ°ç”Ÿæˆè¿”å›ç»“æœ:\n{result}")
    return result


#####################################
# æ–°å¢ï¼šç”Ÿæˆåˆå§‹æç¤ºæ–‡æœ¬ï¼ˆinstructionï¼‰çš„å‡½æ•°
#####################################
def generate_instruction_text(user_query: str) -> str:
    """
    instruction = (
        f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è¾…åŠ©ç³»ç»Ÿï¼Œä¸“é—¨è´Ÿè´£æŒ‡å¯¼å¤§æ¨¡å‹ç”Ÿæˆç¬¦åˆæ–°è¥¿å…°åœ°åŒºè‹±è¯­ä½¿ç”¨ä¹ æƒ¯çš„ç¿»è¯‘ã€‚"
        f"ç”¨æˆ·éœ€è¦ç¿»è¯‘çš„å†…å®¹æ˜¯ï¼š\n{user_query}\n"
        f"è¯·ä½ ç›´æ¥ç»™å‡ºä¸€ä¸ªä¸­æ–‡promptï¼Œç”¨äºæŒ‡å¯¼å¤§æ¨¡å‹è¿›è¡Œä¸Šè¿°ä¸­æ–‡è¯—æ­Œçš„è‹±æ–‡ç¿»è¯‘å·¥ä½œã€‚\n"
        f"ç”Ÿæˆçš„ä¸­æ–‡promptè¦æ±‚ï¼š\n"
        f"1.promptä¸­é¦–å…ˆæŒ‡å‡ºç”¨æˆ·æä¾›çš„ä¸­æ–‡è¯—æ­ŒåŸå†…å®¹ï¼Œå¹¶æå‡ºå°†æ‰€ç»™ä¸­æ–‡è¯—æ­Œç¿»è¯‘æˆè‹±æ–‡è¯—æ­Œè¿™ä¸ªè¦æ±‚ã€‚\n"
        f"2.promptåº”å‚è€ƒç”¨æˆ·ç»™å‡ºçš„ä¸­æ–‡è¯—æ­Œçš„å…·ä½“å†…å®¹ï¼Œé€‰æ‹©æ°å½“çš„è¯­è¨€æè¿°æœ¬é¦–è¯—æ­Œåœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­åº”æ³¨æ„çš„äº‹é¡¹ã€‚"
        f"ï¼ˆä¾‹å¦‚ï¼šå°½é‡ä¼ è¾¾åŸè¯—çš„æ ¸å¿ƒæ€æƒ³ã€æƒ…æ„Ÿå’Œä¸»æ—¨ï¼Œé¿å…æ›²è§£æˆ–è¿‡åº¦å»¶ä¼¸ï¼›"
        f"ä¿ç•™è¯—æ­Œä¸­çš„æ–‡åŒ–æ„è±¡ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„è‹±æ–‡è¡¨è¾¾ï¼›è¯­è¨€è¦æœ‰éŸµå‘³ï¼Œä¿æŒè¯—æ€§è¡¨è¾¾ï¼Œå¦‚èŠ‚å¥ã€æŠ¼éŸµï¼ˆè‹¥èƒ½åšåˆ°ï¼‰ã€ç®€ç»ƒã€ä¿®è¾ç­‰ï¼›"
        f"å¯¹ä¸€äº›ç‰¹æœ‰çš„æ–‡åŒ–å…¸æ•…ã€å†å²äººç‰©æˆ–é£ä¿—éœ€åšæ³¨è§£ã€æ„è¯‘æˆ–æ–‡åŒ–è½¬åŒ–ï¼Œé¿å…è¯»è€…è¯¯è§£ï¼›æ ¹æ®å…·ä½“è¯—å¥çµæ´»å–èˆï¼Œé‡æ„å¢ƒæ—¶æ„è¯‘ï¼Œé‡ç»“æ„æˆ–ä¿®è¾æ—¶åç›´è¯‘ï¼‰\n"
        f"3.promptåº”æŒ‡å‡ºåœ¨ç¿»è¯‘æ—¶è¦ä½“ç°æ–°è¥¿å…°åœ°åŒºè‹±è¯­çš„è¯­è¨€é£æ ¼å’Œæ–‡åŒ–ç‰¹ç‚¹ï¼Œä½†ä½ åœ¨åˆ›å»ºpromptæ—¶åŠ¡å¿…ä¸è¦ç»™å‡ºå…·ä½“ç”¨è¯å»ºè®®ï¼Œæˆ‘æ€•ä½ å­˜åœ¨è¿™æ–¹é¢çš„å¹»è§‰ã€‚"
        f"ï¼ˆå…³äºä»€ä¹ˆæ˜¯æ–°è¥¿å…°è‹±è¯­ï¼Œæˆ‘æŒ‡çš„æ˜¯ä¸‹é¢å¯èƒ½ä¼šæä¾›ç»™ä½ çš„ä¸€éƒ¨åˆ†å…³é”®è¯ï¼Œè¿™äº›å…³é”®è¯å–è‡ªç”±ä¸€æœ¬æ–°è¥¿å…°è‹±è¯­è¯å…¸ç”Ÿæˆçš„å‘é‡æ•°æ®åº“ã€‚"
        f"åœ¨ç¿»è¯‘æœ¬è¯—çš„è¿‡ç¨‹ä¸­ï¼Œæœ‰äº›è‹±æ–‡å•è¯åœ¨æ–°è¥¿å…°è‹±è¯­ä¸­æœ‰ç‰¹æ®Šçš„è¡¨è¾¾ï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰¹æ®Šçš„ä½¿ç”¨æƒ…å†µä»å‘é‡æ•°æ®åº“ä¸­å–å‡ºæ¥ï¼Œ"
        f"ä»¥å…³é”®è¯çš„å½¢å¼å±•ç¤ºï¼Œä½ åœ¨ç”Ÿæˆpromptæ—¶å¯ä»¥å‚è€ƒè¿™äº›è¯çš„é‡Šä¹‰ï¼Œçœ‹æ˜¯å¦å¯ä»¥å°†å…¶èå…¥ä½ çš„è¯—æ­Œç¿»è¯‘ä¸­ã€‚"
        f"è¦å°½å¯èƒ½ä¿ç•™è¯—æ­Œçš„åŸæœ‰å«ä¹‰åŠæ„å¢ƒï¼ŒåŒæ—¶å…¼å…·æ–°è¥¿å…°è‹±è¯­çš„è¡¨è¾¾ç‰¹è‰²ã€‚ï¼‰"
    )
    """
    instruction = (
        f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è¾…åŠ©ç³»ç»Ÿï¼Œä¸“é—¨è´Ÿè´£ç¼–å†™promptæŒ‡å¯¼å¤§æ¨¡å‹è¿›è¡Œä¸­æ–‡è¯—æ­Œå‘æ–°è¥¿å…°è‹±è¯­è¯—æ­Œç¿»è¯‘å·¥ä½œã€‚\n"
        f"è¯—æ­Œå†…å®¹å¦‚ä¸‹ï¼š\n\n{user_query}\n\n"
        f"è¦æ±‚å¦‚ä¸‹ï¼š\n"
        f"1. promptä¸­è¦åŒ…å«è¯—æ­Œå†…å®¹ï¼Œä»¥ä¼ é€’ç»™å¤§æ¨¡å‹ã€‚\n"
        f"2. ç›´æ¥ç»™å‡ºä¸­æ–‡promptã€‚\n"
        f"3. promptåº”å‚è€ƒç”¨æˆ·ç»™å‡ºçš„ä¸­æ–‡è¯—æ­Œçš„å…·ä½“å†…å®¹ï¼Œé€‰æ‹©æ°å½“çš„è¯­è¨€æè¿°æœ¬é¦–è¯—æ­Œåœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­åº”æ³¨æ„çš„äº‹é¡¹ã€‚"
        f"ï¼ˆä¾‹å¦‚ï¼šå°½é‡ä¼ è¾¾åŸè¯—çš„æ ¸å¿ƒæ€æƒ³ã€æƒ…æ„Ÿå’Œä¸»æ—¨ï¼Œé¿å…æ›²è§£æˆ–è¿‡åº¦å»¶ä¼¸ï¼›ä¿ç•™è¯—æ­Œä¸­çš„æ–‡åŒ–æ„è±¡ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„è‹±æ–‡è¡¨è¾¾ï¼›è¯­è¨€è¦æœ‰éŸµå‘³ï¼Œä¿æŒè¯—æ€§è¡¨è¾¾ï¼Œå¦‚èŠ‚å¥ã€æŠ¼éŸµï¼ˆè‹¥èƒ½åšåˆ°ï¼‰ã€ç®€ç»ƒã€ä¿®è¾ç­‰ï¼›"
        f"å¯¹ä¸€äº›ç‰¹æœ‰çš„æ–‡åŒ–å…¸æ•…ã€å†å²äººç‰©æˆ–é£ä¿—éœ€åšæ³¨è§£ã€æ„è¯‘æˆ–æ–‡åŒ–è½¬åŒ–ï¼Œé¿å…è¯»è€…è¯¯è§£ï¼›æ ¹æ®å…·ä½“è¯—å¥çµæ´»å–èˆï¼Œé‡æ„å¢ƒæ—¶æ„è¯‘ï¼Œé‡ç»“æ„æˆ–ä¿®è¾æ—¶åç›´è¯‘ï¼‰\n"
        f"4. Prompt åº”æŒ‡å‡ºç¿»è¯‘è¿‡ç¨‹ä¸­åº”ä½“ç°æ–°è¥¿å…°åœ°åŒºè‹±è¯­çš„è¯­è¨€é£æ ¼ä¸æ–‡åŒ–ç‰¹ç‚¹ã€‚ä½†åœ¨æœ¬é˜¶æ®µç”Ÿæˆæç¤ºè¯æ—¶ï¼Œè¯·å‹¿ç›´æ¥ç»™å‡ºå…·ä½“çš„æ–°è¥¿å…°è‹±è¯­è¯æ±‡ï¼Œä»¥é¿å…å› ç¼ºä¹ä¸Šä¸‹æ–‡äº§ç”Ÿè¯­è¨€å¹»è§‰ã€‚"
        f"æ­¤å¤–ï¼Œè¯·æ³¨æ„ï¼šæœ¬ç³»ç»Ÿåœ¨ç”Ÿæˆæç¤ºè¯ä¹‹åï¼Œå¯èƒ½ä¼šæä¾›ä¸€ç»„ä¸æœ¬è¯—ç›¸å…³çš„æ–°è¥¿å…°è‹±è¯­å…³é”®è¯åŠå…¶é‡Šä¹‰ï¼ˆç”±æ–°è¥¿å…°æœ¬åœ°è¯å…¸æ„å»ºçš„å‘é‡æ•°æ®åº“æ£€ç´¢è€Œå¾—ï¼‰ã€‚è¿™äº›è¯è¯­å°†åœ¨åç»­é˜¶æ®µå±•ç¤ºå¹¶æ³¨å…¥æç¤ºè¯ä¸­ï¼Œä»¥å¢å¼ºæœ¬åœ°åŒ–è¡¨è¾¾ã€‚"
        f"åœ¨ç”Ÿæˆåˆå§‹ prompt æ—¶ï¼Œä½ ä»…éœ€ä¸ºè¿™ç±»åç»­ä¿¡æ¯é¢„ç•™åˆç†çš„è¯­è¨€ç»“æ„ï¼Œè€Œä¸å¿…è‡ªè¡Œè®¾æƒ³è¯æ±‡å†…å®¹ã€‚"
    )
    write_log(f"ç”Ÿæˆåˆå§‹æç¤ºæ–‡æœ¬:\n{instruction}")
    return instruction
def generate_instruction_text_EN(user_query: str) -> str:
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥è¯—å¥ç”Ÿæˆåˆå§‹æç¤ºæ–‡æœ¬ï¼Œä¾›ç”¨æˆ·æŸ¥çœ‹å’Œç¼–è¾‘ã€‚
    instruction = (
        f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è¾…åŠ©ç³»ç»Ÿï¼Œä¸“é—¨è´Ÿè´£æŒ‡å¯¼å¤§æ¨¡å‹ç”Ÿæˆç¬¦åˆæ–°è¥¿å…°è‹±è¯­ä¹ æƒ¯çš„ç¿»è¯‘ã€‚"
        f"ç”¨æˆ·éœ€æ±‚å¦‚ä¸‹ï¼š{user_query}\n"
        f"è¯·ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ç¿»è¯‘æç¤ºï¼Œè¦æ±‚ï¼š\n"
        f"1. ä¿ç•™ä¸­æ–‡å¤è¯—çš„æ„å¢ƒä¸éŸµå¾‹ï¼›\n"
        f"2. ç¿»è¯‘åº”ä½“ç°æ–°è¥¿å…°è‹±è¯­çš„è¯­è¨€é£æ ¼å’Œæ–‡åŒ–ç‰¹ç‚¹ï¼›\n"
        f"è¯·ç›´æ¥ç»™å‡ºpromptç”¨äºæŒ‡å¯¼å¤§æ¨¡å‹è¿›è¡Œä¸Šé¢è¯—æ­Œçš„ç¿»è¯‘å·¥ä½œï¼ˆåŒ…æ‹¬è¯—æ­Œå†…å®¹ã€å¦‚ä½•ç¿»è¯‘å’Œæ³¨æ„äº‹é¡¹ï¼‰ã€‚"
    )
    """
    instruction = (
        f"You are a professional translation support system dedicated to guiding a large language model "
        f"in producing English translations of Chinese poetry that align with the linguistic habits and cultural features of New Zealand English.\n"
        f"The user has submitted the following Chinese poem for translation:\n{user_query}\n"
        f"Please generate an English-language prompt that will instruct the model to translate this Chinese poem into an English poem.\n"
        f"The generated prompt should follow these principles:\n"
        f"1. The prompt must begin by clearly stating the original Chinese poem and explicitly requesting its translation into English poetry.\n"
        f"2. The prompt should reference the specific content of the poem and outline key considerations for the translation. For example:\n"
        f"   - Accurately convey the poem's core ideas, emotional tone, and intended message. Avoid misinterpretation or overextension.\n"
        f"   - Preserve important cultural imagery (such as the moon or concepts of 'home') and choose English expressions that maintain their symbolic meaning.\n"
        f"   - Maintain the poetic flavor of the original text, including rhythm, rhyme (if feasible), conciseness, and rhetorical devices.\n"
        f"   - For culturally specific referencesâ€”such as historical figures, idioms, or customsâ€”consider using annotation, cultural reinterpretation, or interpretive translation to prevent misunderstanding.\n"
        f"   - Flexibly adapt the translation approach line by line: use freer interpretive translation for lines emphasizing imagery and emotion, and more literal translation for lines with structural or rhetorical emphasis.\n"
        f"3. The prompt must emphasize that the translation should reflect the linguistic style and cultural characteristics of New Zealand English. "
        f"However, **do not include specific vocabulary suggestions**, as hallucinations may occur.\n"
        f"(The definition of New Zealand English here refers to a set of keywords that may be provided below, extracted from a vector database built from a New Zealand English dictionary. "
        f"In the translation process, you may consult the definitions of these keywords to determine whether they can be naturally integrated into the poem's translation. "
        f"The goal is to preserve the original meaning and imagery of the poem as much as possible, while expressing it in a style consistent with New Zealand English.)"
    )

    write_log(f"ç”Ÿæˆåˆå§‹ENæç¤ºæ–‡æœ¬:\n{instruction}")
    return instruction

# æŠŠä¸éœ€è¦çš„ chain-of-thought æ ‡è®°è¿‡æ»¤æ‰
def clean_prompt(prompt: str) -> str:
    # ç§»é™¤ chain-of-thought æ ‡è®°ï¼Œæ¯”å¦‚ <think> å’Œ </think>
    prompt = prompt.replace("<think>", "").replace("</think>", "")
    return prompt.strip()


def call_local_qwen_with_instruction(instruction: str, max_new_tokens=1024, min_length=256) -> str:
    """
    ä½¿ç”¨æœ¬åœ° Qwen æ¨¡å‹ç”Ÿæˆæœ€ç»ˆçš„ prompt0ï¼Œ
    è¿™é‡Œçš„è¾“å…¥ä¸ºç”¨æˆ·ç¼–è¾‘åçš„æç¤ºæ–‡æœ¬ï¼ˆinstructionï¼‰ã€‚
    """
    write_log(f"ä½¿ç”¨ç”¨æˆ·ç¼–è¾‘åçš„ instruction è°ƒç”¨æœ¬åœ° Qwen æ¨¡å‹ï¼Œè¾“å…¥:\n{instruction}")
    if not instruction.strip():
        write_log("âŒ [ERROR] instruction ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œæœ¬åœ° Qwen")
        return "âš ï¸ æç¤ºæ–‡æœ¬ä¸ºç©ºï¼Œè¯·å…ˆç”Ÿæˆæˆ–è¾“å…¥æç¤ºæ–‡æœ¬ï¼"
    # ç¡®ä¿ max_new_tokens æ˜¯æ•´æ•°
    try:
        max_new_tokens = int(max_new_tokens)
        min_length = int(min_length)
    except ValueError:
        return "âš ï¸ max_new_tokens æˆ– min_length ä¸æ˜¯æ•´æ•°ï¼Œè¯·æ£€æŸ¥è¾“å…¥ï¼"

    # å¼ºåˆ¶æ€è€ƒæ¨¡å¼ï¼Œå¹¶è¦æ±‚ç›´æ¥ç”Ÿæˆæç¤ºæ–‡æœ¬
    # formatted_instruction = f"<think>\n{instruction}\n</think>\nè¯·ç›´æ¥ç”Ÿæˆç”¨äº DeepSeek API çš„ç¿»è¯‘æç¤ºï¼Œä¸è¦åŒ…å«è§£é‡Šæˆ–æ¨ç†è¿‡ç¨‹ã€‚"
    # result = local_generate(formatted_instruction, max_new_tokens=max_new_tokens, min_length=min_length)
    result = local_generate(instruction, max_new_tokens=max_new_tokens, min_length=min_length)
    # result = clean_prompt(result)  # æ¸…æ´—ä¸éœ€è¦çš„æ ‡è®°
    # ğŸ’¡ åªä¿ç•™ â€œ---â€ ä¸­çš„ä¸»ä½“
    write_log(f"æœ¬åœ° Qwen æ¨¡å‹è¿”å›çš„ prompt0:\n{result}")
    result = strip_dash_block(result)

    return result


#####################################
# 4. å®šä¹‰ Boss æ¨¡å‹ç›¸å…³å‡½æ•°ï¼ˆåç»­äº¤äº’ä¸­ç”¨äºå®¡æ ¸ï¼‰
#####################################
def review_translation_with_boss(prompt_context: str, candidate_translation: str) -> str:
    """
    Boss æ¨¡å‹å¯¹å¤§æ¨¡å‹è¿”å›çš„è¯‘æ–‡è¿›è¡Œå®¡æ ¸ï¼Œå¹¶ç»™å‡ºä¿®æ”¹å»ºè®®æˆ–ç›´æ¥ä¿®æ­£è¯‘æ–‡ã€‚
    """
    instruction = (
        f"ä½ æ˜¯ä¸“ä¸šçš„ç¿»è¯‘ç›‘ç®¡ç³»ç»Ÿï¼Œä¸‹é¢æ˜¯ç”¨æˆ·éœ€æ±‚æç¤ºï¼š\n{prompt_context}\n"
        f"å¤§æ¨¡å‹ç¿»è¯‘ç»“æœå¦‚ä¸‹ï¼š\n{candidate_translation}\n"
        f"è¯·æŒ‡å‡ºè¯‘æ–‡ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶å¦‚æœ‰éœ€è¦ç»™å‡ºæ”¹å†™åçš„ç‰ˆæœ¬ï¼Œè¦æ±‚è¯‘æ–‡æ—¢ä¿ç•™åŸè¯—æ„å¢ƒï¼Œåˆç¬¦åˆæ–°è¥¿å…°è‹±è¯­ä¹ æƒ¯ã€‚\n"
        "åªéœ€ç»™å‡ºç¿»è¯‘é—®é¢˜åˆ†æä¸æ”¹è¿›å»ºè®®ï¼Œæ— éœ€ç»™å‡ºæ”¹åè¯‘æ–‡å’Œä¿®æ”¹è¯´æ˜"
    )
    review_feedback = local_generate(instruction, max_new_tokens=4000, min_length=256)
    write_log(
        f"å®¡æ ¸ç¿»è¯‘è°ƒç”¨ï¼š\nä½¿ç”¨ prompt:\n{prompt_context}\nå€™é€‰ç¿»è¯‘:\n{candidate_translation}\nå®¡æ ¸åé¦ˆ:\n{review_feedback}")
    return review_feedback


#####################################
# 5. DeepSeek API è°ƒç”¨å‡½æ•°
#####################################
def call_deepseek_api(boss_prompt: str) -> str:
    """
    ä½¿ç”¨ DeepSeek API è°ƒç”¨å¤§æ¨¡å‹ï¼Œä¼ å…¥ Boss æ¨¡å‹ç”Ÿæˆçš„ prompt è·å–ç¿»è¯‘ç»“æœã€‚
    """
    write_log(f"è°ƒç”¨ DeepSeek APIï¼Œè¯·æ±‚å†…å®¹:\n{boss_prompt}")
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",  # æˆ– "deepseek-reasoner"
            messages=[
                {"role": "system", "content": "You are a helpful assistant specialized in translating Chinese poetry."},
                {"role": "user", "content": boss_prompt},
            ],
            temperature=1.4,
            stream=False
        )
        translation = response.choices[0].message.content
        write_log(f"DeepSeek API è¿”å›ç¿»è¯‘:\n{translation}")
        return translation
    except Exception as e:
        write_log(f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {str(e)}")
        print(f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {str(e)}")
        return f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {str(e)}"


#####################################
# 6. Demo æµç¨‹ï¼ˆä¾›å‘½ä»¤è¡Œæµ‹è¯•ä½¿ç”¨ï¼Œå¯åœ¨ Web è°ƒç”¨ä¸­ä¸ä½¿ç”¨ï¼‰
#####################################
def translation_workflow(user_input: str, max_rounds: int = 2):
    """
    æ•´ä½“æµç¨‹ï¼š
      1. ç”Ÿæˆåˆå§‹æç¤ºæ–‡æœ¬ï¼ˆinstructionï¼‰ï¼›
      2. ä½¿ç”¨ç”¨æˆ·ç¼–è¾‘åçš„æç¤ºè°ƒç”¨æœ¬åœ° Qwen æ¨¡å‹ç”Ÿæˆ prompt0ï¼›
      3. è°ƒç”¨ DeepSeek API è·å–åˆæ­¥ç¿»è¯‘ï¼›
      4. Boss æ¨¡å‹å®¡æ ¸è¯‘æ–‡ï¼Œè‹¥åé¦ˆä¸­å»ºè®®ä¿®æ”¹ï¼Œåˆ™è¿­ä»£é‡æ–°ç”Ÿæˆæç¤ºåè°ƒç”¨ DSï¼›
      5. è¿”å›æœ€ç»ˆè¯‘æ–‡ã€‚
    """
    write_log(f"==== å¼€å§‹äº¤äº’æµç¨‹ï¼Œç”¨æˆ·è¾“å…¥éœ€æ±‚ ====\n{user_input}")
    print(f"\n=== ç”¨æˆ·è¾“å…¥éœ€æ±‚: {user_input} ===\n")

    # ç”Ÿæˆåˆå§‹æç¤ºæ–‡æœ¬ä¾›ç”¨æˆ·ç¼–è¾‘
    initial_instruction = generate_instruction_text(user_input)
    # æ­¤å¤„ç”¨æˆ·å¯ç¼–è¾‘åæäº¤ç»™æœ¬åœ° Qwen æ¨¡å‹
    prompt0 = call_local_qwen_with_instruction(initial_instruction)
    print(f"[Boss æ¨¡å‹] ç”Ÿæˆçš„ Prompt0:\n{prompt0}\n")

    ds_translation = call_deepseek_api(prompt0)
    print(f"[DeepSeek å¤§æ¨¡å‹] è¿”å›çš„ç¿»è¯‘:\n{ds_translation}\n")

    review_feedback = review_translation_with_boss(prompt0, ds_translation)
    print(f"[Boss æ¨¡å‹] å®¡æŸ¥åé¦ˆ:\n{review_feedback}\n")

    # ç¤ºä¾‹ï¼šè‹¥åé¦ˆä¸­åŒ…å«ä¿®æ”¹å»ºè®®ï¼Œåˆ™ä½¿ç”¨åé¦ˆç”Ÿæˆä¿®è®¢åçš„ prompt é‡æ–°è°ƒç”¨ DS
    if any(keyword in review_feedback for keyword in ["æ”¹å†™", "éœ€ä¿®æ”¹", "å»ºè®®"]):
        revised_prompt = "è¯·æ ¹æ®ä»¥ä¸‹åé¦ˆé‡æ–°ç”Ÿæˆç¿»è¯‘æç¤ºï¼š" + review_feedback
        write_log("å‘ç°ä¿®æ”¹å»ºè®®ï¼Œé‡æ–°ç”Ÿæˆæç¤ºå¹¶è°ƒç”¨ DSã€‚")
        ds_translation = call_deepseek_api(revised_prompt)
        print(f"[DeepSeek å¤§æ¨¡å‹] ä¿®è®¢åè¿”å›çš„ç¿»è¯‘:\n{ds_translation}\n")
    write_log("==== äº¤äº’æµç¨‹ç»“æŸ ====")
    return ds_translation

def get_local_model():
    global model
    return model

def get_local_tokenizer():
    global tokenizer
    return tokenizer
    
if __name__ == "__main__":
    # ç®€å•å‘½ä»¤è¡Œç¤ºä¾‹ï¼ˆä¾›è°ƒè¯•ä½¿ç”¨ï¼‰
    user_query = (
        "è¯·æŠŠè¿™é¦–ä¸­å›½å¤è¯—ç¿»è¯‘æˆç¬¦åˆæ–°è¥¿å…°è‹±è¯­ä¹ æƒ¯çš„è¯—æ„æ–‡æœ¬ï¼š\n"
        "ã€Šé™å¤œæ€ã€‹ - æç™½\n"
        "åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚"
    )
    final_result = translation_workflow(user_query, max_rounds=3)
    print("=== æœ€ç»ˆè¯‘æ–‡ ===")
    print(final_result)
    log_file.close()
