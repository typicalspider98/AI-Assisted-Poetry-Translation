# æ”¹è¿›åçš„ semantic_helper.pyï¼ˆé‡‡ç”¨å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼Œè§£å†³å¤šè¿›ç¨‹å†²çªé—®é¢˜ï¼‰

import json
import re
from typing import List, Dict, Tuple

import gradio as gr
from gradio_checkboxgroupmarkdown import CheckboxGroupMarkdown

import redis
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from FlagEmbedding import FlagReranker
from transformers import AutoModelForSequenceClassification

from translation_logic import call_local_qwen_with_instruction
from translation_logic import write_log

# === Redis é…ç½® ===
redis_vec = redis.Redis(host="localhost", port=6379, db=2)
redis_dict = redis.Redis(host="localhost", port=6379, db=0)

# æ”¯æŒå¤šä¸ªåµŒå…¥æ¨¡å‹è·¯å¾„
EMBEDDING_MODELS = {
    2: "../../VectorDatabase/models/bge-m3",
}
_loaded_models = {}

# === å»¶è¿ŸåŠ è½½ reranker æ¨¡å‹ ===
_flag_reranker = None
_gte_tokenizer = None
_gte_model = None

# === åŠŸèƒ½æ¨¡å— ===
def build_keyword_prompt(poem_text: str, max_new_tokens: int = 128) -> str:
    '''
    prompt = (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ–‡å¤å…¸è¯—æ­Œä¸è‹±è¯­æ–‡åŒ–çš„ç¿»è¯‘é¡¾é—®ã€‚\n"
        "è¯·æ ¹æ®ä¸‹åˆ—è¯—æ­Œå†…å®¹æå–5~8ä¸ªç”¨äºæŒ‡å¯¼è‹±æ–‡ç¿»è¯‘çš„å…³é”®è¯æˆ–æ„è±¡çŸ­è¯­ï¼Œ\n"
        "è¯·ä¸è¦é€å¥ç…§æ¬åŸè¯—å¥ï¼Œè€Œæ˜¯æå–å…¶ä¸­å¯ç”¨äºç¿»è¯‘æ—¶çš„æ„è±¡ã€æ–‡åŒ–æ¦‚å¿µæˆ–æ ¸å¿ƒä¸»é¢˜è¯ã€‚\n"
        "å¹¶è¿”å› JSON æ ¼å¼ï¼Œå…³é”®è¯åº”å…·æœ‰ç¿»è¯‘ä»·å€¼ä¸æ–‡åŒ–è±¡å¾æ€§ã€‚\n"
        "JSON ç¤ºä¾‹æ ¼å¼å¦‚ä¸‹ï¼š\n"
        "{\n  \"keywords\": [\n    \"moonlight\",\n    \"bed\",\n    \"homesickness\"\n  ]\n}\n\n"
        f"è¯—æ­ŒåŸæ–‡ï¼š{poem_text}"
    ).strip()
    '''
    prompt = (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ–‡å¤å…¸è¯—æ­Œä¸è‹±è¯­æ–‡åŒ–çš„ç¿»è¯‘é¡¾é—®ã€‚\n"
        "è¯·æ ¹æ®ä¸‹åˆ—è¯—æ­Œå†…å®¹åŠé•¿åº¦æå–é€‚é‡çš„å…³é”®è¯ï¼Œç”¨äºæŒ‡å¯¼è‹±æ–‡ç¿»è¯‘ã€‚\n"
        "è¦æ±‚:\n"
        "1.ä¸è¦é€å¥ç…§æ¬åŸè¯—å¥ï¼Œç»“åˆè¯—æ­ŒåŸæ„å’Œæ„å¢ƒï¼Œæå–å…¶ä¸­å¯ç”¨äºç¿»è¯‘çš„æ ¸å¿ƒä¸»é¢˜è¯ï¼ŒåŒ…æ‹¬åŠ¨è¯ã€åè¯ã€å½¢å®¹è¯ã€å‰¯è¯ç­‰ï¼Œä»¥åŠæ„è±¡å’Œæ–‡åŒ–æ¦‚å¿µã€‚\n"
        "2.åœ¨ä¸ç ´åè¯—æ­Œæœ¬æ„å’Œæ„å¢ƒçš„å‰æä¸‹ï¼Œæå–çš„ä¸»é¢˜è¯æ˜¯æŒ‰ä¸­æ–‡è¯—æ­ŒéŸµå¾‹å’Œè¯­ä¹‰åœé¡¿åˆ’åˆ†å‡ºçš„æœ€å°è¯­ç´ å’Œè¯è¯­ï¼Œç‰¹æ®Šæƒ…å†µä¸‹ä¸ºä¸ç ´åè¯—æ­Œæ•´ä½“å«ä¹‰ä¹Ÿå¯ä»¥æ˜¯ä¸­æ–‡çŸ­è¯­ã€‚\n"
        "3.ä¸»é¢˜è¯åŒ…æ‹¬æ‰€æœ‰æ„è±¡ï¼Œå¹¶æœ€ç»ˆä»¥å‡†ç¡®ç®€æ˜çš„è‹±æ–‡å•è¯å±•ç¤ºã€‚\n"
        "4.å¹¶è¿”å› JSON æ ¼å¼ï¼Œå…³é”®è¯åº”å…·æœ‰ç¿»è¯‘ä»·å€¼ä¸æ–‡åŒ–è±¡å¾æ€§ã€‚\n"
        "JSON ç¤ºä¾‹æ ¼å¼å¦‚ä¸‹ï¼š\n"
        "{\n  \"keywords\": [\n    \"word1: English simple explanation\",\n    \"word2: English simple explanation\",\n    \"word3: English simple explanation\"\n  ]\n}\n\n"
        f"è¯—æ­ŒåŸæ–‡ï¼š{poem_text}"
    ).strip()
    return prompt
def build_keyword_prompt_EN(poem_text: str, max_new_tokens: int = 128) -> str:
    '''
    prompt = (
        "ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ–‡å¤å…¸è¯—æ­Œä¸è‹±è¯­æ–‡åŒ–çš„ç¿»è¯‘é¡¾é—®ã€‚\n"
        "è¯·æ ¹æ®ä¸‹åˆ—è¯—æ­Œå†…å®¹æå–5~8ä¸ªç”¨äºæŒ‡å¯¼è‹±æ–‡ç¿»è¯‘çš„å…³é”®è¯æˆ–æ„è±¡çŸ­è¯­ï¼Œ\n"
        "è¯·ä¸è¦é€å¥ç…§æ¬åŸè¯—å¥ï¼Œè€Œæ˜¯æå–å…¶ä¸­å¯ç”¨äºç¿»è¯‘æ—¶çš„æ„è±¡ã€æ–‡åŒ–æ¦‚å¿µæˆ–æ ¸å¿ƒä¸»é¢˜è¯ã€‚\n"
        "å¹¶è¿”å› JSON æ ¼å¼ï¼Œå…³é”®è¯åº”å…·æœ‰ç¿»è¯‘ä»·å€¼ä¸æ–‡åŒ–è±¡å¾æ€§ã€‚\n"
        "JSON ç¤ºä¾‹æ ¼å¼å¦‚ä¸‹ï¼š\n"
        "{\n  \"keywords\": [\n    \"moonlight\",\n    \"bed\",\n    \"homesickness\"\n  ]\n}\n\n"
        f"è¯—æ­ŒåŸæ–‡ï¼š{poem_text}"
    ).strip()
    '''
    prompt = (
        f"You are a translation advisor well-versed in classical Chinese poetry and English cultural expression.\n"
        f"Based on the following poem and its length, extract a suitable set of thematic keywords to guide the English translation process.\n"
        f"Requirements:\n"
        f"1. Do not simply extract phrases directly from each line of the original poem.\n"
        f"2. Instead, analyze the overall meaning and imagery to identify core thematic words that can assist in capturing the essence of the poem in translation. "
        f"These may include verbs, nouns, adjectives, adverbs, as well as symbolic imagery and culturally relevant concepts.\n"
        f"3. Extract keywords according to natural pauses and semantic breaks in the poem's structure, following the rhythm and meaning of the original Chinese.\n"
        f"4. In special cases, short Chinese phrases may be accepted as a single unit if breaking them apart would compromise the poem's overall meaning or imagery.\n"
        f"5. Include all key symbols and imagery from the poem, and present the extracted thematic keywords as accurate, concise English words.\n"
        f"6. Return the results in JSON format. The selected keywords should carry both translational value and cultural symbolism.\n"
        f"Expected JSON format:\n"
        "{\n  \"keywords\": [\n    \"word1: English simple explanation\",\n    \"word2: English simple explanation\",\n    \"word3: English simple explanation\"\n  ]\n}\n\n"
        f"Original Poem:{poem_text}"
    ).strip()
    return prompt

def extract_keywords_with_llm(prompt_text: str, max_new_tokens: int = 128) -> str:
    response = call_local_qwen_with_instruction(prompt_text, max_new_tokens=max_new_tokens)
    
    # è®°å½•å®Œæ•´å“åº”åˆ°æ—¥å¿—
    try:
        # from translation_logic import write_log
        write_log("[å…³é”®è¯æç¤ºå®Œæ•´è¿”å›]\n" + response)
    except Exception:
        print("[æ—¥å¿—è®°å½•å¤±è´¥] æœªèƒ½è°ƒç”¨ write_log")

    # æå– markdown ä¸­çš„ JSON å†…å®¹
    try:
        match = re.search(r"```json\s*({[\s\S]*?})\s*", response)
        # match = re.search(r"```json\\s*(\\{[\\s\\S]*?\\})\\s*```", response)
        if match:
            return match.group(1)
            write_log(f"[JSONè§£ææˆåŠŸ] {match.group(1)}")
        else:
            # return "{}"
            return response
    except Exception as e:
        print(f"[JSONè§£æå¤±è´¥] {e}")
        write_log(f"[JSONè§£æå¤±è´¥] {e}")
        return "{}"
    return response


# === å‘é‡ç”Ÿæˆ ===

def get_embedding(text: str, model_id: int = 2):
    if model_id not in _loaded_models:
        path = EMBEDDING_MODELS.get(model_id)
        if not path:
            raise ValueError(f"âŒ æœªçŸ¥çš„åµŒå…¥æ¨¡å‹ç¼–å·: {model_id}")
        tokenizer = AutoTokenizer.from_pretrained(path)
        model = AutoModel.from_pretrained(path).to("cuda")
        _loaded_models[model_id] = (tokenizer, model)

    tokenizer, model = _loaded_models[model_id]
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    tokens = {k: v.to(model.device) for k, v in tokens.items()}

    with torch.no_grad():
        output = model(**tokens)
    embedding = output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().astype(np.float32)
    return embedding

# === TopKæ£€ç´¢ä¸é‡æ’åº ===
# === å»¶è¿ŸåŠ è½½ reranker è¾…åŠ©å‡½æ•° ===

def custom_sigmoid(x, temperature=1.5):
    return 1 / (1 + np.exp(-temperature * x))

def get_flag_reranker():
    global _flag_reranker
    if _flag_reranker is None:
        print("[åŠ è½½] FlagEmbedding reranker...")
        _flag_reranker = FlagReranker("../models/bge-reranker-v2-m3", use_fp16=True)
    return _flag_reranker

def get_gte_reranker():
    global _gte_tokenizer, _gte_model
    if _gte_tokenizer is None or _gte_model is None:
        print("[åŠ è½½] GTE reranker...")
        _gte_tokenizer = AutoTokenizer.from_pretrained("../models/gte-multilingual-reranker-base")
        _gte_model = AutoModelForSequenceClassification.from_pretrained(
            "../models/gte-multilingual-reranker-base", trust_remote_code=True, torch_dtype=torch.float16
        ).eval().to("cuda")
    return _gte_tokenizer, _gte_model
def rerank_with_flag(pairs: List[List[str]]) -> List[float]:
    model = get_flag_reranker()
    scores = model.compute_score(pairs, normalize=False)
    return [custom_sigmoid(x, temperature=2.0) for x in scores]

def rerank_with_gte(pairs: List[List[str]]) -> List[float]:
    tokenizer, model = get_gte_reranker()
    with torch.no_grad():
        inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to("cuda")
        logits = model(**inputs).logits.view(-1).float()
        return [custom_sigmoid(x.item(), temperature=2.0) for x in logits]

def search_topk_similar_batch(queries: List[str], top_k: int = 6, model_id: int = 2):
    merged_query = ", ".join(queries)
    query_vector = get_embedding(merged_query, model_id)
    query_vector = np.asarray(query_vector, dtype=np.float32).flatten()
    query_norm = np.linalg.norm(query_vector)

    words = redis_vec.keys("*")
    similarities = []
    for word in words:
        stored_vector = np.frombuffer(redis_vec.get(word), dtype=np.float32).flatten()
        stored_norm = np.linalg.norm(stored_vector)

        if stored_norm == 0 or query_norm == 0:
            similarity = 0.0
        else:
            similarity = float(np.dot(query_vector, stored_vector)) / (query_norm * stored_norm)

        similarities.append((word.decode("utf-8"), similarity))

    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]



def search_topk_with_reranker(queries: List[str], top_k: int = 6, model_id: int = 2) -> List[Dict]:
    """
    å·¦å³ä½ç½®é¢ å€’ç‰ˆï¼šå·¦è¾¹æ˜¯kwï¼ˆå…³é”®è¯ï¼‰ï¼Œå³è¾¹æ˜¯RedisæŸ¥åˆ°çš„è¯æ¡+é‡Šä¹‰
    """

    results = []

    for raw_kw in queries:
        # === æå–å…³é”®è¯ï¼ˆåªè¦å†’å·å‰çš„ï¼‰
        if ":" in raw_kw:
            kw = raw_kw.split(":")[0].strip()
        else:
            kw = raw_kw.strip()

        # [1] ç”¨kwå»RedisæŸ¥TopK
        topk = search_topk_similar_batch([kw], top_k=top_k*2, model_id=model_id)

        pairs = []
        redis_keys = []
        cos_sims = []
        passages = []

        for key, cos_sim in topk:
            word_base = key.rsplit("-", 1)[0]
            meaning = "(æ— è§£é‡Š)"

            if redis_dict.exists(key):
                try:
                    entry = json.loads(redis_dict.get(key))
                    meaning = entry.get("meaning", meaning)
                except:
                    pass

            message = f"{word_base}: {meaning}" if meaning != "(æ— è§£é‡Š)" else word_base

            # âœ… æ”¹è¿™é‡Œï¼šå·¦è¾¹æ˜¯ kwï¼Œå³è¾¹æ˜¯ message
            pairs.append([kw, message])

            passages.append(message)
            redis_keys.append(key)
            cos_sims.append(cos_sim)

        # [2] rerankeræ‰“åˆ†
        print("\n--- å½“å‰pairså†…å®¹ ---")
        for p in pairs:
            print(f"å·¦è¾¹(query=kw): {p[0]} || å³è¾¹(message): {p[1]}")
        print("--- ç»“æŸ ---\n")

        scores_flag = rerank_with_flag(pairs)
        scores_gte = rerank_with_gte(pairs)

        # [3] ä¿å­˜æ‰“åˆ†
        for i in range(len(pairs)):
            results.append({
                "keyword": kw,  # æŸ¥è¯¢è¯
                "redis_key": redis_keys[i],
                "cosine_similarity": cos_sims[i],
                "flag_score": scores_flag[i],
                "gte_score": scores_gte[i],
                "final_score": max(scores_flag[i], scores_gte[i])
            })

    # [4] æ’åº
    results.sort(key=lambda x: x["final_score"], reverse=True)

    return results[:top_k]



def search_topk_with_reranker_demo0(queries: List[str], top_k: int = 6, model_id: int = 2) -> List[Dict]:
    merged_query = ", ".join(queries)
    query_vector = get_embedding(merged_query, model_id)
    query_norm = np.linalg.norm(query_vector)

    words = redis_vec.keys("*")
    similarities = []
    for word in words:
        stored_vector = np.frombuffer(redis_vec.get(word), dtype=np.float32)
        stored_norm = np.linalg.norm(stored_vector)
        if stored_norm == 0 or query_norm == 0:
            similarity = 0.0
        else:
            similarity = float(np.dot(query_vector, stored_vector)) / (query_norm * stored_norm)
        similarities.append((word.decode("utf-8"), similarity))

    similarities.sort(key=lambda x: x[1], reverse=True)
    topk = similarities[:top_k * 2]

    pairs = []
    for key, _ in topk:
        word = key.rsplit("-", 1)[0]
        '''
        meaning = "(æ— è§£é‡Š)"
        if redis_dict.exists(key):
            try:
                entry = json.loads(redis_dict.get(key))
                meaning = entry.get("meaning", meaning)
            except:
                pass
        passage = f"{word}: {meaning}"
        '''
        passage = word
        pairs.append([merged_query, passage])

    scores_flag = rerank_with_flag(pairs)
    scores_gte = rerank_with_gte(pairs)

    results = []
    for i, (key, cos_sim) in enumerate(topk):
        results.append({
            "redis_key": key,
            "cosine_similarity": cos_sim,
            "flag_score": scores_flag[i],
            "gte_score": scores_gte[i],
            "final_score": max(scores_flag[i], scores_gte[i])
        })

    results.sort(key=lambda x: x["final_score"], reverse=True)
    return results[:top_k]

# === æŸ¥è¯¢ç›¸å…³è¯å¹¶ç»„ç»‡ç»“æœ ===

def query_related_terms_from_redis(json_text: str, top_k: int = 6, model_id: int = 2) -> List[Dict]:
    if not json_text.strip():
        return []
    try:
        keywords = json.loads(json_text).get("keywords", [])
    except Exception as e:
        print(f"[è§£æå…³é”®è¯JSONå¤±è´¥] {e}")
        return []

    all_data = []
    for kw in keywords:
        related_items = []
        topk = search_topk_with_reranker([kw], top_k=top_k, model_id=model_id)
        for item in topk:
            redis_key = item["redis_key"]
            word_base = redis_key.rsplit("-", 1)[0]
            meaning, example_text = "(æ— è§£é‡Š)", ["(æ— ä¾‹å¥)"]

            dict_data_raw = redis_dict.get(redis_key)
            if dict_data_raw:
                try:
                    dict_data = json.loads(dict_data_raw)
                    meaning = dict_data.get("meaning", meaning)
                    examples = dict_data.get("examples", [])
                    if isinstance(examples, list):
                        example_text = examples[:2]
                except Exception:
                    pass

            related_items.append({
                "word": word_base,
                "explanation": meaning,
                "examples": example_text,
                "redis_key": redis_key,
                "cosine_similarity": item["cosine_similarity"],
                "flag_score": item["flag_score"],
                "gte_score": item["gte_score"],   
                "final_score": item["final_score"]
            })

        all_data.append({
            "keyword": kw,
            "topk": related_items
        })

    return all_data

# === æ¸²æŸ“å…³é”®è¯ç›¸å…³è¯ä¸ºCheckboxå±•ç¤º ===
def weather_icon(score: float) -> str:
    if score >= 0.70:
        return "â˜€ï¸ (Excellent)"
    elif score >= 0.60:
        return "ğŸŒ¤ï¸ (Good)"
    elif score >= 0.50:
        return "â˜ï¸ (Moderate)"
    elif score >= 0.40:
        return "ğŸŒ§ï¸ (Weak)"
    else:
        return "ğŸŒ©ï¸ (Poor)"
def render_checkbox_groups_by_keyword(all_data: list):
    updates = []
    for i, item in enumerate(all_data):
        keyword = item.get("keyword", f"å…³é”®è¯{i+1}")
        topk = item.get("topk", [])

        choices = []
        for j, entry in enumerate(topk):
            word = entry["word"]
            explanation = entry["explanation"]
            examples = entry["examples"]

            cosine = entry.get("cosine_similarity", 0.0)
            flag = entry.get("flag_score", 0.0)
            gte = entry.get("gte_score", 0.0)
            final = entry.get("final_score", 0.0)
            
            # æŠŠå››ä¸ªåˆ†æ•°ä¸€èµ·å±•ç¤º
            # score = entry.get("score", 0.0)
            # score_display = f"â­ï¸ ç›¸ä¼¼åº¦ï¼š{score:.3f}" if score > 0.75 else f"ç›¸ä¼¼åº¦ï¼š{score:.3f}"
            '''
            score_display = (
                f"ä½™å¼¦ç›¸ä¼¼åº¦ (cos): {cosine:.3f}, "
                f"Flagå¾—åˆ†: {flag:.3f}, "
                f"GTEå¾—åˆ†: {gte:.3f}, "
                f"æœ€ç»ˆå¾—åˆ†: {final:.3f}"
            )
            '''
            score_display = (
                "|  cos-Sim | Flag-Score | GTE-Score | **Final Score** |\n"
                "| :---: | :---: | :---: | :---: |\n"
                # f"| {cosine:.3f} | {flag:.3f} | {gte:.3f} | {'â­ï¸ ' if final > 0.6 else ''}{final:.3f} |"
                f"| {cosine:.3f} | {flag:.3f} | {gte:.3f} | **{final:.3f}** |"
            )

    
            choices.append({
                "id": f"{i}_{j}",
                "title": explanation,
                "content": f"### {word} {weather_icon(final)}\n{score_display}\n\n" + "\n".join(f"- {ex}" for ex in examples),
                "selected": False
            })

        half = (len(choices) + 1) // 2
        left_choices = choices[:half]
        right_choices = choices[half:]

        updates.append([
            gr.update(choices=left_choices, visible=True, label=f"{keyword}"),
            gr.update(choices=right_choices, visible=True, label=f"{keyword}")
        ])

    while len(updates) < 50:
        updates.append([
            gr.update(choices=[], visible=False, label=""),
            gr.update(choices=[], visible=False, label="")
        ])

    return [item for pair in updates for item in pair]

# ï¼ˆå…¶ä»–collect_grouped_markdown_selection, update_accordion_labels, inject_keywords_into_promptä¿æŒä¸å˜ï¼‰


def collect_grouped_markdown_selection(*args) -> str:
    """
    å‚æ•°ç»“æ„ï¼š
    - å‰ 2*N ä¸ªæ˜¯ CheckboxGroupMarkdown çš„ .value
    - æœ€åä¸€ä¸ªæ˜¯ all_dataï¼ˆåŒ…å«æ‰€æœ‰å…³é”®è¯ç»„åŠå…¶ TopK é‡Šä¹‰ä¿¡æ¯ï¼‰
    """
    *group_values, all_data = args
    result = {}

    for group_index in range(len(group_values) // 2):
        selected_left = group_values[group_index * 2] or []
        selected_right = group_values[group_index * 2 + 1] or []
        selected_ids = selected_left + selected_right

        if group_index >= len(all_data):
            continue

        keyword = all_data[group_index].get("keyword", f"å…³é”®è¯{group_index+1}")
        topk_items = all_data[group_index].get("topk", [])

        id_to_entry = {
            f"{group_index}_{j}": entry for j, entry in enumerate(topk_items)
        }

        result[keyword] = []
        for id_ in selected_ids:
            entry = id_to_entry.get(id_)
            if entry:
                result[keyword].append({
                    "word": entry["word"],
                    "explanation": entry["explanation"],
                    "examples": entry["examples"]
                })

    return json.dumps(result, ensure_ascii=False, indent=2)


def update_accordion_labels(all_related_data):
    return [
        gr.Accordion.update(label=f"å…³é”®è¯ï¼š{group['keyword']}", visible=True)
        for group in all_related_data
    ] + [gr.Accordion.update(visible=False)] * (50 - len(all_related_data))


def inject_keywords_into_prompt(prompt: str, selected_json: str) -> str:
    try:
        selected_items = json.loads(selected_json)
    except Exception as e:
        return prompt + "\n\nâš ï¸ [å…³é”®è¯æ³¨å…¥å¤±è´¥] JSON è§£æé”™è¯¯"

    context_parts = []
    for keyword, entries in selected_items.items():
        if not entries:
            continue  # âœ… è·³è¿‡æ²¡æœ‰é€‰ä¸­ä»»ä½•ç›¸å…³è¯çš„å…³é”®è¯
        terms = [f"{entry['word']} ({entry['explanation']})" for entry in entries]
        context_parts.append(f"{keyword}: " + ", ".join(terms))

    if not context_parts:
        return prompt + "\n\nâš ï¸ [æç¤º] å½“å‰æœªé€‰æ‹©ä»»ä½•å…³é”®è¯é‡Šä¹‰ï¼Œæœªè¿›è¡Œæ³¨å…¥"

    context = "ä»¥ä¸‹æ˜¯ä¸æœ¬é¦–è¯—æ­Œç¿»è¯‘ç›¸å…³çš„æ–°è¥¿å…°è‹±è¯­å…³é”®è¯åŠé‡Šä¹‰ï¼Œå¯ä¾›å‚è€ƒï¼š\n" + "\n".join(context_parts)
    return prompt.strip() + "\n\n" + context


if __name__ == "__main__":
    print("this is not the file you should run.\nGo find web_interface.py")