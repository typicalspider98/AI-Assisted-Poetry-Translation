# nz_phrase_matcher.py

import os
import json
import ahocorasick
import pickle
import re
import pandas as pd


# === é¢„å¤„ç†ç¿»è¯‘æ–‡æœ¬ï¼Œå¤„ç†æ‰€æœ‰æ ¼ + æ¸…ç†æ ‡ç‚¹ ===
def normalize_text_for_matching(text: str) -> str:
    text = text.lower()
    text = re.sub(r"\b(\w+)[â€™']s\b", r"\1", text)  # æ‰€æœ‰æ ¼å»é™¤
    text = re.sub(r"[^\w\s]", "", text)  # å»é™¤æ ‡ç‚¹
    return text


# === ä» JSON æ–‡ä»¶å¤¹æå–ä¸»è¯æ¡ ===
def get_words_from_json(json_folder="nz_dictionary_jsons"):
    all_words = set()
    for file in os.listdir(json_folder):
        if file.endswith(".json"):
            with open(os.path.join(json_folder, file), "r", encoding="utf-8") as f:
                data = json.load(f)
                for entry in data:
                    word = entry.get("word", "").strip().lower()
                    if len(word) >= 3 and word.isalpha():  # è¿‡æ»¤çŸ­è¯ã€æ— æ•ˆè¯
                        all_words.add(word)
    return all_words


# === æ„å»ºå¹¶ä¿å­˜ AC è‡ªåŠ¨æœº ===
def build_and_save_ac_automaton(json_folder="nz_dictionary_jsons", output_path="ac_nzdict.pkl"):
    phrases = get_words_from_json(json_folder)
    print(f"âœ… ä» JSON è·å–ä¸»è¯æ¡æ•°é‡: {len(phrases)}")

    A = ahocorasick.Automaton()
    for phrase in phrases:
        A.add_word(phrase, phrase)
    A.make_automaton()

    with open(output_path, "wb") as f:
        pickle.dump(A, f)
    print(f"âœ… è‡ªåŠ¨æœºå·²ä¿å­˜è‡³: {output_path}")


# === åŠ è½½æŒä¹…åŒ– AC è‡ªåŠ¨æœº ===
def load_ac_automaton(pkl_path="ac_nzdict.pkl"):
    if not os.path.exists(pkl_path):
        raise FileNotFoundError("âŒ è‡ªåŠ¨æœºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ„å»ºä¸€æ¬¡ï¼")
    with open(pkl_path, "rb") as f:
        A = pickle.load(f)
    print("âœ… è‡ªåŠ¨æœºåŠ è½½æˆåŠŸ")
    return A


# === åŒ¹é…è¾“å…¥æ–‡æœ¬ä¸­å‘½ä¸­çš„ NZ çŸ­è¯­è¯æ¡ ===
def match_nz_phrases(text: str, automaton) -> list:
    cleaned = normalize_text_for_matching(text)
    hits = set()
    for _, phrase in automaton.iter(cleaned):
        if re.search(rf"\b{re.escape(phrase)}\b", cleaned):
            hits.add(phrase)
    return sorted(hits)


def process_translation_table(filepath, ac_machine):
    df = pd.read_excel(filepath) if filepath.endswith(".xlsx") else pd.read_csv(filepath, encoding='gbk')
    poems = df.columns.tolist()
    results = {}

    for col in df.columns:
        poem_name = col
        versions = df[col].dropna().tolist()  # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜ï¼Œåé¢æ˜¯ç¿»è¯‘ç‰ˆæœ¬

        version_results = []
        for i, version in enumerate(versions):
            hits = match_nz_phrases(version, ac_machine)
            version_results.append({
                "version_index": i + 1,
                "hit_count": len(hits),
                "hits": hits
            })

        results[poem_name] = version_results

    return results


if __name__ == "__main__":
    pkl_path = "ac_nzdict1.pkl"
    json_folder = "nz_dictionary_jsons"
    input_table = "translation_versions1.csv"  # <-- 

    # è‡ªåŠ¨æ„å»ºæˆ–åŠ è½½è‡ªåŠ¨æœº
    if not os.path.exists(pkl_path):
        print("ğŸ”§ æœªæ£€æµ‹åˆ°å·²ä¿å­˜çš„è‡ªåŠ¨æœºï¼Œæ­£åœ¨åˆå§‹åŒ–æ„å»º...")
        build_and_save_ac_automaton(json_folder=json_folder, output_path=pkl_path)
    else:
        print("ğŸ“ è‡ªåŠ¨æœºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»ºã€‚")

    ac_machine = load_ac_automaton(pkl_path)

    # æ‰§è¡Œè¡¨æ ¼å¤„ç†
    results = process_translation_table(input_table, ac_machine)

    # æ‰“å°ç»“æœ
    for poem, versions in results.items():
        print(f"\nğŸ“œ è¯—æ­Œ: {poem}")
        for res in versions:
            print(f"  ç‰ˆæœ¬{res['version_index']} å‘½ä¸­: {res['hit_count']} ä¸ªè¯æ¡ â†’ {res['hits']}")